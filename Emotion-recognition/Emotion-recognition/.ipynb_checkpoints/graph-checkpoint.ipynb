{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2965efdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cudaSetDevice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcudaSetDevice\u001b[49m(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cudaSetDevice' is not defined"
     ]
    }
   ],
   "source": [
    "cudaSetDevice(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580c445",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f38533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import visualization\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from Transformer_Explainability.BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
    "from Transformer_Explainability.BERT_explainability.modules.BERT.BertForSequenceClassification import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from Transformer_Explainability.BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee17a241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c5f1cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import clip\n",
    "from transformers import BertTokenizer\n",
    "from Transformer_Explainability.BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
    "from src.data_cleaning.script import *\n",
    "from src.config.config import *\n",
    "from src.emotion.go_emotion import *\n",
    "from src.emotion.model import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f302a823",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = \"for_labeling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa536244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  \n",
    "import pandas as pd\n",
    "DATA = f'C:/Users/Charlie/Desktop/Database/BERT-Sentiment-Analysis-Reddit-API/DataCleaning/sqldf/{subreddit}.csv'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a82ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "297bfbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.num_labels:  28\n"
     ]
    }
   ],
   "source": [
    "emoji = pd.read_csv('C:/Users/Charlie/Desktop/Database/BERT-Sentiment-Analysis-Reddit-API/Emotion-recognition/Emotion-recognition/emoji_mapping_table.csv')\n",
    "emoji_lst = list(emoji['Emoji'])\n",
    "try:\n",
    "    mturk_df = pd.read_csv(DATA,sep=\",\", encoding='cp1252',on_bad_lines='skip')\n",
    "except:\n",
    "    mturk_df = pd.read_csv(DATA,sep=\",\", encoding='utf-8',on_bad_lines='skip')\n",
    "mturk_df\n",
    "image_url_list = []\n",
    "title_list = []\n",
    "comments_dict = defaultdict(list)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else print('hello'))\n",
    "wt = WordNetTagger()\n",
    "bert_model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\").to(device)\n",
    "\n",
    "bert_model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "explanations = Generator(bert_model)\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "110f5db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.num_labels:  28\n",
      "[('[CLS]', 0.0), ('This', 0.2158810943365097), ('movie', 0.0899757519364357), ('was', 0.11041862517595291), ('the', 0.35469499230384827), ('best', 0.7750786542892456), ('movie', 0.0709317997097969), ('I', 0.0), ('have', 0.061757270246744156), ('ever', 1.0), ('seen', 0.2806461751461029), ('!', 0.003720361040905118), ('some', 0.20789656043052673), ('scenes', 0.2965109944343567), ('were', 0.23326589167118073), ('ridiculous', 0.6767276525497437), (',', 0.0019471825798973441), ('but', 0.5572431087493896), ('acting', 0.3725723624229431), ('was', 0.07197000086307526), ('great', 0.8070467710494995), ('.', 0.06248350813984871), ('[SEP]', 0.01159625593572855)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> This                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 62%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seen                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> scenes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ridiculous                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> acting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>0 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>1.00</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> This                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 62%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 50%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seen                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> !                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> some                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> scenes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> were                    </font></mark><mark style=\"background-color: hsl(120, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ridiculous                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> but                    </font></mark><mark style=\"background-color: hsl(120, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> acting                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\").to(\"cuda\")\n",
    "text_batch = [\"This movie was the best movie I have ever seen! some scenes were ridiculous, but acting was great.\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt')\n",
    "input_ids = encoding['input_ids'].to(\"cuda\")\n",
    "attention_mask = encoding['attention_mask'].to(\"cuda\")\n",
    "\n",
    "# true class is positive - 1\n",
    "true_class = 1\n",
    "\n",
    "# generate an explanation for the input\n",
    "expl = explanations.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=0)[0]\n",
    "# normalize scores\n",
    "expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
    "\n",
    "# get the model classification\n",
    "output = torch.nn.functional.softmax(model(input_ids=input_ids, attention_mask=attention_mask)[0], dim=-1)\n",
    "classification = output.argmax(dim=-1).item()\n",
    "# get class name\n",
    "class_name = classifications[classification]\n",
    "# if the classification is negative, higher explanation scores are more negative\n",
    "# flip for visualization\n",
    "if class_name == \"NEGATIVE\":\n",
    "  expl *= (-1)\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "print([(tokens[i], expl[i].item()) for i in range(len(tokens))])\n",
    "vis_data_records = [visualization.VisualizationDataRecord(\n",
    "                                expl,\n",
    "                                output[0][classification],\n",
    "                                classification,\n",
    "                                true_class,\n",
    "                                true_class,\n",
    "                                1,       \n",
    "                                tokens,\n",
    "                                1)]\n",
    "visualization.visualize_text(vis_data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf33aeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SubmissionID</th>\n",
       "      <th>SubmissionTitle</th>\n",
       "      <th>CommentID</th>\n",
       "      <th>Comment</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>Images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300001</td>\n",
       "      <td>Putin, Me, photography</td>\n",
       "      <td>1</td>\n",
       "      <td>I liked the one of Putin himself buried in there</td>\n",
       "      <td>art</td>\n",
       "      <td>https://i.redd.it/7ycc263aruj81.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300001</td>\n",
       "      <td>Putin, Me, photography</td>\n",
       "      <td>2</td>\n",
       "      <td>Me “Ugh, I hate when people mark posts NSFW fo...</td>\n",
       "      <td>art</td>\n",
       "      <td>https://i.redd.it/7ycc263aruj81.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300001</td>\n",
       "      <td>Putin, Me, photography</td>\n",
       "      <td>3</td>\n",
       "      <td>Love that they all appear to be flacid</td>\n",
       "      <td>art</td>\n",
       "      <td>https://i.redd.it/7ycc263aruj81.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300001</td>\n",
       "      <td>Putin, Me, photography</td>\n",
       "      <td>4</td>\n",
       "      <td>I’m honestly more impressed by the collection....</td>\n",
       "      <td>art</td>\n",
       "      <td>https://i.redd.it/7ycc263aruj81.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300001</td>\n",
       "      <td>Putin, Me, photography</td>\n",
       "      <td>5</td>\n",
       "      <td>Well i can't unzoom in but this was probably t...</td>\n",
       "      <td>art</td>\n",
       "      <td>https://i.redd.it/7ycc263aruj81.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251296</th>\n",
       "      <td>332038</td>\n",
       "      <td>Colossus of the Alps</td>\n",
       "      <td>55</td>\n",
       "      <td>The last one looks like a trip to Auschwitz</td>\n",
       "      <td>wallpaper</td>\n",
       "      <td>https://i.redd.it/k2pt3pao31f31.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251297</th>\n",
       "      <td>332038</td>\n",
       "      <td>Colossus of the Alps</td>\n",
       "      <td>56</td>\n",
       "      <td>I remember being about 4 years old, standing u...</td>\n",
       "      <td>wallpaper</td>\n",
       "      <td>https://i.redd.it/k2pt3pao31f31.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251298</th>\n",
       "      <td>332038</td>\n",
       "      <td>Colossus of the Alps</td>\n",
       "      <td>57</td>\n",
       "      <td>I'm 60, remember going to get seat belts put i...</td>\n",
       "      <td>wallpaper</td>\n",
       "      <td>https://i.redd.it/k2pt3pao31f31.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251299</th>\n",
       "      <td>332038</td>\n",
       "      <td>Colossus of the Alps</td>\n",
       "      <td>58</td>\n",
       "      <td>Just think, folks; the reason why we don’t do ...</td>\n",
       "      <td>wallpaper</td>\n",
       "      <td>https://i.redd.it/k2pt3pao31f31.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2251300</th>\n",
       "      <td>332038</td>\n",
       "      <td>Colossus of the Alps</td>\n",
       "      <td>59</td>\n",
       "      <td>That was my car seat growning up! Lot less car...</td>\n",
       "      <td>wallpaper</td>\n",
       "      <td>https://i.redd.it/k2pt3pao31f31.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2251301 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         SubmissionID         SubmissionTitle  CommentID  \\\n",
       "0              300001  Putin, Me, photography          1   \n",
       "1              300001  Putin, Me, photography          2   \n",
       "2              300001  Putin, Me, photography          3   \n",
       "3              300001  Putin, Me, photography          4   \n",
       "4              300001  Putin, Me, photography          5   \n",
       "...               ...                     ...        ...   \n",
       "2251296        332038    Colossus of the Alps         55   \n",
       "2251297        332038    Colossus of the Alps         56   \n",
       "2251298        332038    Colossus of the Alps         57   \n",
       "2251299        332038    Colossus of the Alps         58   \n",
       "2251300        332038    Colossus of the Alps         59   \n",
       "\n",
       "                                                   Comment  subreddit  \\\n",
       "0         I liked the one of Putin himself buried in there        art   \n",
       "1        Me “Ugh, I hate when people mark posts NSFW fo...        art   \n",
       "2                   Love that they all appear to be flacid        art   \n",
       "3        I’m honestly more impressed by the collection....        art   \n",
       "4        Well i can't unzoom in but this was probably t...        art   \n",
       "...                                                    ...        ...   \n",
       "2251296        The last one looks like a trip to Auschwitz  wallpaper   \n",
       "2251297  I remember being about 4 years old, standing u...  wallpaper   \n",
       "2251298  I'm 60, remember going to get seat belts put i...  wallpaper   \n",
       "2251299  Just think, folks; the reason why we don’t do ...  wallpaper   \n",
       "2251300  That was my car seat growning up! Lot less car...  wallpaper   \n",
       "\n",
       "                                      Images  \n",
       "0        https://i.redd.it/7ycc263aruj81.jpg  \n",
       "1        https://i.redd.it/7ycc263aruj81.jpg  \n",
       "2        https://i.redd.it/7ycc263aruj81.jpg  \n",
       "3        https://i.redd.it/7ycc263aruj81.jpg  \n",
       "4        https://i.redd.it/7ycc263aruj81.jpg  \n",
       "...                                      ...  \n",
       "2251296  https://i.redd.it/k2pt3pao31f31.jpg  \n",
       "2251297  https://i.redd.it/k2pt3pao31f31.jpg  \n",
       "2251298  https://i.redd.it/k2pt3pao31f31.jpg  \n",
       "2251299  https://i.redd.it/k2pt3pao31f31.jpg  \n",
       "2251300  https://i.redd.it/k2pt3pao31f31.jpg  \n",
       "\n",
       "[2251301 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mturk_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "460a5b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300000/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n",
      "300001/300002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m save \u001b[38;5;241m=\u001b[39m mturk_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmissionID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubmissionTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImages\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m     18\u001b[0m comment \u001b[38;5;241m=\u001b[39m m_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComment\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 20\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcap_and_comments\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     22\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m clip_model\u001b[38;5;241m.\u001b[39mencode_image(image)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\clip\\clip.py:222\u001b[0m, in \u001b[0;36mtokenize\u001b[1;34m(texts, context_length, truncate)\u001b[0m\n\u001b[0;32m    220\u001b[0m sot_token \u001b[38;5;241m=\u001b[39m _tokenizer\u001b[38;5;241m.\u001b[39mencoder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|startoftext|>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    221\u001b[0m eot_token \u001b[38;5;241m=\u001b[39m _tokenizer\u001b[38;5;241m.\u001b[39mencoder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|endoftext|>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 222\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m [[sot_token] \u001b[38;5;241m+\u001b[39m _tokenizer\u001b[38;5;241m.\u001b[39mencode(text) \u001b[38;5;241m+\u001b[39m [eot_token] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m packaging\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m packaging\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.8.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    224\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(all_tokens), context_length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\clip\\clip.py:222\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    220\u001b[0m sot_token \u001b[38;5;241m=\u001b[39m _tokenizer\u001b[38;5;241m.\u001b[39mencoder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|startoftext|>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    221\u001b[0m eot_token \u001b[38;5;241m=\u001b[39m _tokenizer\u001b[38;5;241m.\u001b[39mencoder[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<|endoftext|>\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 222\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m [[sot_token] \u001b[38;5;241m+\u001b[39m \u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m [eot_token] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m packaging\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mparse(torch\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m packaging\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.8.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    224\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(all_tokens), context_length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\clip\\simple_tokenizer.py:123\u001b[0m, in \u001b[0;36mSimpleTokenizer.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    122\u001b[0m     bpe_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 123\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mwhitespace_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasic_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpat, text):\n\u001b[0;32m    125\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbyte_encoder[b] \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m token\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\clip\\simple_tokenizer.py:57\u001b[0m, in \u001b[0;36mwhitespace_clean\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwhitespace_clean\u001b[39m(text):\n\u001b[1;32m---> 57\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43ms+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\regex\\regex.py:278\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags, pos, endpos, concurrent, timeout, ignore_unused, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost (or rightmost with a\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03mreverse pattern) non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03mreplacement repl. repl can be either a string or a callable; if a string,\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mbackslash escapes in it are processed; if a callable, it's passed the match\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03mobject and must return a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m    277\u001b[0m pat \u001b[38;5;241m=\u001b[39m _compile(pattern, flags, ignore_unused, kwargs, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_to_csv = []\n",
    "start = 300001\n",
    "count = start-1\n",
    "end = 300002\n",
    "for index, m_row in mturk_df.loc[(mturk_df['SubmissionID'] >= start) & (mturk_df['SubmissionID'] <= end)].iterrows():\n",
    "    print(f'{count}/{end}')\n",
    "    if m_row['SubmissionID'] != count:\n",
    "        urllib.request.urlretrieve(m_row['Images'], 'tmp.png')\n",
    "        image = preprocess(Image.open(\"tmp.png\")).unsqueeze(0).to(device)\n",
    "        count = m_row['SubmissionID']\n",
    "    else:\n",
    "        image = image\n",
    "        \n",
    "    sid = m_row['SubmissionID']\n",
    "    if start <= sid <= end:\n",
    "        cap_and_comments = [''.join(t for t in m_row['SubmissionTitle'] if str(t) not in emoji_lst)]\n",
    "        save = mturk_df[[\"SubmissionID\", \"SubmissionTitle\", \"Images\"]]\n",
    "        comment = m_row['Comment']\n",
    "        \n",
    "        text = clip.tokenize(cap_and_comments).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(image)\n",
    "            text_features = clip_model.encode_text(text)\n",
    "            logits_per_image, logits_per_text = clip_model(image, text)\n",
    "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "        sorted_idx = [int(i[0]) for i in sorted(enumerate(probs[0]), key=lambda x: x[1], reverse=True)]\n",
    "        post_labels = []\n",
    "        \n",
    "        comment_clean = comment.replace(\"'\", \"\")\n",
    "\n",
    "        inputs = tokenizer(comment_clean, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = bert_model(**inputs)\n",
    "        scores = 1 / (1 + torch.exp(-outputs[0]))  # Sigmoid\n",
    "        threshold = .5\n",
    "        comment_labels = []\n",
    "        for idx, score in enumerate(scores[0]):\n",
    "            if score > threshold:\n",
    "                label = bert_model.config.id2label[idx]\n",
    "                comment_labels.append((label,float(score)))\n",
    "        if len(comment_labels) == 0:\n",
    "            idx,score = max(enumerate(scores[0]))\n",
    "            label = bert_model.config.id2label[idx]\n",
    "            comment_labels.append((label,float(score)))\n",
    "        post_labels.append(comment_labels)\n",
    "    insert_row = [count,comment,post_labels[0]]\n",
    "    save_to_csv.append(insert_row)\n",
    "\n",
    "df = pd.DataFrame(save_to_csv)\n",
    "df.to_csv(f'C:/Users/Charlie/Desktop/Database/BERT-Sentiment-Analysis-Reddit-API/DataCleaning/sqldf/tags2/start_{start}_end_{end}.csv', index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "623c53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'C:/Users/Charlie/Desktop/Database/BERT-Sentiment-Analysis-Reddit-API/DataCleaning/sqldf/tags2/start_{start}_end_{end}.csv', index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b290195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300001</td>\n",
       "      <td>I liked the one of Putin himself buried in there</td>\n",
       "      <td>[(neutral, 0.9705334901809692)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300001</td>\n",
       "      <td>Me “Ugh, I hate when people mark posts NSFW fo...</td>\n",
       "      <td>[(anger, 0.9462706446647644)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300001</td>\n",
       "      <td>Love that they all appear to be flacid</td>\n",
       "      <td>[(love, 0.9484241604804993)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300001</td>\n",
       "      <td>I’m honestly more impressed by the collection....</td>\n",
       "      <td>[(admiration, 0.9545209407806396), (optimism, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300001</td>\n",
       "      <td>Well i can't unzoom in but this was probably t...</td>\n",
       "      <td>[(neutral, 0.5297912955284119)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>300002</td>\n",
       "      <td>The title is the only thing thats really setti...</td>\n",
       "      <td>[(approval, 0.9055155515670776)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>300002</td>\n",
       "      <td>anyone else reminded of their parents?</td>\n",
       "      <td>[(neutral, 0.9907659292221069)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>300002</td>\n",
       "      <td>Hardcore agenda pushing in the comments. Much ...</td>\n",
       "      <td>[(admiration, 0.9908730983734131)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>300002</td>\n",
       "      <td>It should have a half dozen kids peering out t...</td>\n",
       "      <td>[(neutral, 0.9987926483154297)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>300002</td>\n",
       "      <td>I feel like this is bordering on  territory.</td>\n",
       "      <td>[(approval, 0.9946132302284241)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0                                                  1  \\\n",
       "0    300001   I liked the one of Putin himself buried in there   \n",
       "1    300001  Me “Ugh, I hate when people mark posts NSFW fo...   \n",
       "2    300001             Love that they all appear to be flacid   \n",
       "3    300001  I’m honestly more impressed by the collection....   \n",
       "4    300001  Well i can't unzoom in but this was probably t...   \n",
       "..      ...                                                ...   \n",
       "217  300002  The title is the only thing thats really setti...   \n",
       "218  300002             anyone else reminded of their parents?   \n",
       "219  300002  Hardcore agenda pushing in the comments. Much ...   \n",
       "220  300002  It should have a half dozen kids peering out t...   \n",
       "221  300002       I feel like this is bordering on  territory.   \n",
       "\n",
       "                                                     2  \n",
       "0                      [(neutral, 0.9705334901809692)]  \n",
       "1                        [(anger, 0.9462706446647644)]  \n",
       "2                         [(love, 0.9484241604804993)]  \n",
       "3    [(admiration, 0.9545209407806396), (optimism, ...  \n",
       "4                      [(neutral, 0.5297912955284119)]  \n",
       "..                                                 ...  \n",
       "217                   [(approval, 0.9055155515670776)]  \n",
       "218                    [(neutral, 0.9907659292221069)]  \n",
       "219                 [(admiration, 0.9908730983734131)]  \n",
       "220                    [(neutral, 0.9987926483154297)]  \n",
       "221                   [(approval, 0.9946132302284241)]  \n",
       "\n",
       "[222 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848399b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(f'C:/Users/Charlie/Desktop/Database/BERT-Sentiment-Analysis-Reddit-API/DataCleaning/sqldf/tags2/{subreddit}_tags.csv', index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52772873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384f972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128ee83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "c = Counter()\n",
    "with open(f\"json_data2/{subreddit}.json\", \"w\") as outfile:\n",
    "    for file in os.listdir('json_data2'):\n",
    "        if file.startswith('emo_distribution_start'):\n",
    "            with open(f'json_data2/{file}') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "                for dct in data:\n",
    "                    print('\\n------------------')\n",
    "                    print('submission_id: ', dct['submission_id'])\n",
    "                    print('number_of_comments: ', dct['number_of_comments'])\n",
    "                    print('relevance_score: ', dct['relevance_score'])\n",
    "                    c += Counter([j for i in dct['emotion'] for j in i])\n",
    "                    print('emotion: ', c)\n",
    "                    print('------------------\\n')\n",
    "    json.dump(c, outfile)\n",
    "\n",
    "with open(f'json_data2/{subreddit}.json') as f:\n",
    "    data = json.load(f)\n",
    "    data = {k: v for k, v in data.items() if k != 'neutral'}\n",
    "    data = dict(sorted(data.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # Create a bar plot\n",
    "    plt.barh(list(data.keys()), data.values(), edgecolor='black', linewidth=1.2)\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xticks(size=12)\n",
    "    plt.yticks(size=12)\n",
    "    plt.ylabel('Emotion', fontsize=20)\n",
    "    plt.xlabel('Count', fontsize=20)\n",
    "    # plt.title('Histogram')\n",
    "    plt.savefig(f'json_data2/{subreddit}.png')\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f429f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866fd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e737fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lable_table = []\n",
    "save_to_json = []\n",
    "start = 0\n",
    "end = 2\n",
    "for index, m_row in mturk_df.loc[(mturk_df['index'] >= start) & (mturk_df['index'] <= end)].iterrows():\n",
    "    try:\n",
    "        urllib.request.urlretrieve(m_row['image_link'], 'tmp.png')\n",
    "        image = preprocess(Image.open(\"tmp.png\")).unsqueeze(0).to(device)\n",
    "        sid = m_row['index']\n",
    "        if start < sid < end:\n",
    "            cap_and_comments = [''.join(t for t in m_row['title'] if str(t) not in emoji_lst)]\n",
    "            save = mturk_df[[\"index\", \"title\", \"image_link\"]]\n",
    "            comments_list = mturk_df.drop(['index', 'title', 'image_link'], axis=1)\n",
    "            comments = comments_list.iloc[index]\n",
    "            for entry in comments:\n",
    "                if str(entry) not in emoji_lst and str(entry) != '0' and str(entry) != 'fill' and str(entry) != '/':\n",
    "                    comment = ''.join(entry)\n",
    "                    cap_and_comments.append(comment[:100])\n",
    "                # print('idx: ', idx, 'comment: ', comment[:200])\n",
    "            text = clip.tokenize(cap_and_comments).to(device)\n",
    "            with torch.no_grad():\n",
    "                image_features = clip_model.encode_image(image)\n",
    "\n",
    "                text_features = clip_model.encode_text(text)\n",
    "\n",
    "                logits_per_image, logits_per_text = clip_model(image, text)\n",
    "                probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "            # print(probs)\n",
    "            sorted_idx = [int(i[0]) for i in sorted(enumerate(probs[0]), key=lambda x: x[1], reverse=True)]\n",
    "            post_labels = []\n",
    "            for s in np.array(cap_and_comments)[sorted_idx]:\n",
    "                inputs = tokenizer(s, return_tensors=\"pt\")\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = bert_model(**inputs)\n",
    "                scores = 1 / (1 + torch.exp(-outputs[0]))  # Sigmoid\n",
    "                threshold = .5\n",
    "\n",
    "                comment_labels = []\n",
    "                for idx, score in enumerate(scores[0]):\n",
    "                    if score > threshold:\n",
    "                        label = bert_model.config.id2label[idx]\n",
    "                        comment_labels.append(label)\n",
    "                        lable_table.append((save['image_link'][0],s,label))\n",
    "                post_labels.append(comment_labels)\n",
    "\n",
    "            print('relevance score: ', probs[0][sorted_idx])\n",
    "            print('emotion: ', post_labels)\n",
    "\n",
    "            save_to_json.append(\n",
    "                {'submission_id': sid,\n",
    "                 'number_of_comments': len(lable_table),\n",
    "                 'relevance_score': ' '.join(str(i) for i in probs[0][sorted_idx]),\n",
    "                 'emotion': post_labels}\n",
    "            )\n",
    "            # Write the data to a JSON file\n",
    "            with open(f'json_data2/emo_distribution_start{start}_end{end}.json', 'w') as f:\n",
    "                json.dump(save_to_json, f)\n",
    "            print(f'submission {sid} saved to json!!')\n",
    "\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
