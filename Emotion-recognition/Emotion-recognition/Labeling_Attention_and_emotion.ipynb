{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95279a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/hila-chefer/Transformer-Explainability.git\n",
    "\n",
    "import os\n",
    "os.chdir(f'./Transformer-Explainability')\n",
    "\n",
    "!pip install -r requirements.txt\n",
    "!pip install captum\n",
    "!pip install captum==0.6.0\n",
    "!pip install matplotlib==3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import visualization\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from Transformer_Explainability.BERT_explainability.modules.BERT.ExplanationGenerator import Generator\n",
    "from Transformer_Explainability.BERT_explainability.modules.BERT.BertForSequenceClassification import BertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import urllib\n",
    "import clip\n",
    "from src.data_cleaning.script import *\n",
    "from src.config.config import *\n",
    "from src.emotion.go_emotion import *\n",
    "from src.emotion.model import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import traceback\n",
    "#############################\n",
    "subreddit = \"for_labeling\"\n",
    "from pathlib import Path  \n",
    "import pandas as pd\n",
    "DATA = f'C:/Users/Charlie/Desktop/Database/BERT-Sentiment-Analysis-Reddit-API/DataCleaning/sqldf/{subreddit}.csv'  \n",
    "\n",
    "#############################\n",
    "emoji = pd.read_csv('C:/Users/Charlie/Desktop/Database/BERT-Sentiment-Analysis-Reddit-API/Emotion-recognition/Emotion-recognition/emoji_mapping_table.csv')\n",
    "emoji_lst = list(emoji['Emoji'])\n",
    "try:\n",
    "    mturk_df = pd.read_csv(DATA,sep=\",\", encoding='cp1252',on_bad_lines='skip')\n",
    "except:\n",
    "    mturk_df = pd.read_csv(DATA,sep=\",\", encoding='utf-8',on_bad_lines='skip')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else print('hello'))\n",
    "wt = WordNetTagger()\n",
    "bert_model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\").to(device)\n",
    "\n",
    "bert_model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"monologg/bert-base-cased-goemotions-original\")\n",
    "explanations = Generator(bert_model)\n",
    "\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "model = BertForMultiLabelClassification.from_pretrained(\"monologg/bert-base-cased-goemotions-original\").to(\"cuda\")\n",
    "\n",
    "\n",
    "##################################\n",
    "save_to_csv = []\n",
    "save_to_json = []\n",
    "start = 300241\n",
    "end = 300280\n",
    "count = start-1\n",
    "for index, m_row in mturk_df.loc[(mturk_df['SubmissionID'] >= start) & (mturk_df['SubmissionID'] <= end)].iterrows():\n",
    "    commentid = m_row['CommentID']\n",
    "    sid = m_row['SubmissionID']\n",
    "    try:\n",
    "        if sid != count:\n",
    "            urllib.request.urlretrieve(m_row['Images'], 'tmp.png')\n",
    "            image = preprocess(Image.open(\"tmp.png\")).unsqueeze(0).to(device)\n",
    "            count = m_row['SubmissionID']\n",
    "            print(f'{count}/{end}')\n",
    "        else:\n",
    "            image = image\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    print(f'{count}/{end}:Comment{commentid}')\n",
    "\n",
    "    if start <= sid <= end:\n",
    "        cap_and_comments = [''.join(t for t in m_row['SubmissionTitle'] if str(t) not in emoji_lst)]\n",
    "        comment = m_row['Comment']\n",
    "        \n",
    "        text = clip.tokenize(cap_and_comments).to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(image)\n",
    "            text_features = clip_model.encode_text(text)\n",
    "            logits_per_image, logits_per_text = clip_model(image, text)\n",
    "            probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "        sorted_idx = [int(i[0]) for i in sorted(enumerate(probs[0]), key=lambda x: x[1], reverse=True)]\n",
    "        post_labels = []\n",
    "        \n",
    "        comment_clean = comment.replace(\"'\", \"\")\n",
    "        if len(comment_clean) >= 512:\n",
    "            comment_clean = comment_clean[:511]\n",
    "\n",
    "        inputs = tokenizer(comment_clean, return_tensors=\"pt\")\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = bert_model(**inputs)\n",
    "        scores = 1 / (1 + torch.exp(-outputs[0]))  # Sigmoid\n",
    "        threshold = .3\n",
    "        comment_labels = []\n",
    "        for idx, score in enumerate(scores[0]):\n",
    "            if score > threshold:\n",
    "                label = bert_model.config.id2label[idx]\n",
    "                comment_labels.append((label,float(score)))\n",
    "        if len(comment_labels) == 0:\n",
    "            idx,score = max(enumerate(scores[0]))\n",
    "            label = bert_model.config.id2label[idx]\n",
    "            comment_labels.append((label,float(score)))\n",
    "        post_labels.append(comment_labels)\n",
    "    insert_row = [count,comment,post_labels[0]]\n",
    "    save_to_csv.append(insert_row)\n",
    "    \n",
    "    #attention mapper\n",
    "    text_batch = comment_clean\n",
    "    encoding = tokenizer(text_batch, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids'].to(\"cuda\")\n",
    "    attention_mask = encoding['attention_mask'].to(\"cuda\")\n",
    "\n",
    "    # true class is positive - 1\n",
    "    true_class = 1\n",
    "\n",
    "    # generate an explanation for the input\n",
    "    expl = explanations.generate_LRP(input_ids=input_ids, attention_mask=attention_mask, start_layer=0)[0]\n",
    "    # normalize scores\n",
    "    expl = (expl - expl.min()) / (expl.max() - expl.min())\n",
    "\n",
    "    # get the model classification\n",
    "    output = torch.nn.functional.softmax(model(input_ids=input_ids, attention_mask=attention_mask)[0], dim=-1)\n",
    "    classification = output.argmax(dim=-1).item()\n",
    "    # get class name\n",
    "    class_name = classifications[classification]\n",
    "    # if the classification is negative, higher explanation scores are more negative\n",
    "    # flip for visualization\n",
    "    if class_name == \"NEGATIVE\":\n",
    "        expl *= (-1)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.flatten())\n",
    "    save_tokens = [(tokens[i], expl[i].item()) for i in range(len(tokens))]\n",
    "    save_to_json.append(\n",
    "        {'submission_id': count,\n",
    "         'comment_id': commentid,\n",
    "         'comment': comment,\n",
    "         'emotion': post_labels[0],\n",
    "         'attention': save_tokens})\n",
    "    \n",
    "with open(f'json_data2/emo_distribution_start{start}_end{count}.json', 'w') as f:\n",
    "    json.dump(save_to_json, f)\n",
    "    print('saved to json!')\n",
    "    \n",
    "df = pd.DataFrame(save_to_csv)\n",
    "df.to_csv(f'C:/Users/Charlie/Desktop/Database/BERT-Sentiment-Analysis-Reddit-API/DataCleaning/sqldf/tags2/start_{start}_end_{count}.csv', index=False, encoding='utf8')\n",
    "save_to_csv = None\n",
    "save_to_json = None\n",
    "df = None\n",
    "\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
